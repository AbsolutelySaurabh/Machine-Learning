{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "linear-regression-scratch.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbsolutelySaurabh/Machine-Learning/blob/master/ai-coursework/Day-2%20-%20Linear%20regression/linear_regression_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA8sMF_RtujD",
        "colab_type": "text"
      },
      "source": [
        "Our goal is to learn a linear model $\\hat{y}$ that models $y$ given $X$.\n",
        "\n",
        "$\\hat{y} = XW + b$\n",
        "\n",
        "$\\hat{y}$ = predictions | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n",
        "\n",
        "$X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n",
        "\n",
        "$W$ = weights | $\\in \\mathbb{R}^{DX1}$\n",
        "\n",
        "$b$ = bias | $\\in \\mathbb{R}^{1}$\n",
        "\n",
        "Objective: Use inputs $X$ to predict the output $\\hat{y}$ using a linear model. The model will be a line of best fit that minimizes the distance between the predicted (model's output) and target (ground truth) values. Training data $(X, y)$ is used to train the model and learn the weights $W$ using gradient descent.\n",
        "\n",
        "Advantages:\n",
        "* Computationally simple.\n",
        "* Highly interpretable.\n",
        "* Can account for continuous and categorical features.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "* The model will perform well only when the data is linearly separable (for classification).\n",
        "* Usually not used for classification and only for regression.\n",
        "\n",
        "Miscellaneous: You can also use linear regression for binary classification tasks where if the predicted continuous value is above a threshold, it belongs to a certain class. But we will cover better techniques for classification in future lessons and will focus on linear regression for continuous regression tasks only.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "cEgPKi4itujE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "SXWVLjo6tujM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('../input/auto-insurance-in-sweden-small-dataset/insurance.csv',skiprows=list(range(0,5)),header=None,names=['claims','total_payment'])\n",
        "dataset.head()\n",
        "plt.scatter(dataset.claims, dataset.total_payment)\n",
        "\n",
        "X = dataset[['claims']]\n",
        "y = dataset[['total_payment']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kk0-q3stujP",
        "colab_type": "text"
      },
      "source": [
        "### Batch gradient Descent.\n",
        "\n",
        "Explaination:\n",
        "\n",
        "Linear Model\n",
        "The idea of linear regression is to fit a line to a set of points. So let's use the line function given by:\n",
        "> f(x)=y=mx+b\n",
        " \n",
        "where m is the slope and b is our y intercept, or for a more general form (multiple variables)\n",
        "h(x)=θoxo+θ1x1+θ2x2+...+θnxn\n",
        " \n",
        "such that for a single variable where n = 1,\n",
        "> h(x)= 0xo + 01x1\n",
        " \n",
        "when xo=1 , \n",
        "0 : theta\n",
        "\n",
        "So we can say : \n",
        "\n",
        "> H = X(dot)0\n",
        " \n",
        "where theta is our parameters (slope and intercept) and h(x) is our hypothesis or predicted value.\n",
        "\n",
        "Note : follow matrix rule.,\n",
        "> To multiply matrices A[m*n] to B[a*b] , n == a\n",
        "\n",
        "\n",
        "refer : https://www.kaggle.com/kennethjohn/linear-regression-from-scratch\n",
        "\n",
        "goku mohandas : https://github.com/madewithml/lessons/blob/master/notebooks/02_Basics/01_Linear_Regression/01_PT_Linear_Regression.ipynb\n",
        "\n",
        "formula : \n",
        "\n",
        "\n",
        "Gradients\n",
        "**Calculate the gradient of loss $J(\\theta)$ w.r.t to the model weights.**\n",
        "> $J(\\theta) = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2  = \\frac{1}{N}\\sum_i (y_i - X_iW)^2 $\n",
        "\n",
        "> $\\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW) X_i = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i) X_i$\n",
        "> $\\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW)1 = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i)1$\n",
        "\n",
        "ignoring constant '2'and applying leanring rate,\n",
        "\n",
        "> $\\frac{\\partial{J}}{\\partial{W}} = \\frac{learning_rate}{N} \\sum_i (y_i - X_iW)1 = \\frac{alpha}{N} \\sum_i (\\hat{y}_i - y_i)1$\n",
        "\n",
        "W: theta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c_7E8eWZtujR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# perofrming the linear regression\n",
        "class LinearRegression(object):\n",
        "    \"\"\" Performs LR using batch gradient descnet\"\"\"\n",
        "    \n",
        "    def __init__(self, X, y, alpha = 0.01, n_iterations = 10000):\n",
        "        \"\"\"initialises variables\n",
        "        \n",
        "        Parameters : \n",
        "        y : numpy array like, output / dependent varaible #population\n",
        "        X : numpy array like, input / independent variables #profit\n",
        "        alpha : float, int. Learning rate\n",
        "        n_iterations : Number of maximum iterations to perform gradient descent\n",
        "        \"\"\"\n",
        "        \n",
        "        self.y = y;\n",
        "        \n",
        "        ''' \n",
        "        Till now : X = n x  1 matrix\n",
        "        But we want in the form thst of we multiplr it with theta(0) matrix it should b in the form of   \n",
        "                                        theta(not) + theta(1)(x)\n",
        "        So, for the abouve coefficient to come for each Xi value, we need to include 1 '''\n",
        "\n",
        "        self.X = self._hstack_one(X) # this process will give us the n x 2 matrix with 1 in 0th column\n",
        "        \n",
        "        print(self.X.shape[1]) # X.shape[1] --> number of columns\n",
        "        self.thetas = np.zeros((self.X.shape[1], 1))  # since X = n x 2, hence thetas = 2 x any for matric multiply rule\n",
        "        \n",
        "        self.n_rows = self.X.shape[0]\n",
        "        \n",
        "        self.alpha = alpha\n",
        "        self.n_iterations = n_iterations\n",
        "        \n",
        "        print(\"Cost before fitting: {0:.2f}\".format(self.cost()))\n",
        "        \n",
        "        \n",
        "    @staticmethod\n",
        "    def _hstack_one(input_matrix):\n",
        "        \"\"\"Horizontally stack a col of 1s for the coeff of bias terms\n",
        "\n",
        "        Parameters : \n",
        "        input_matrix : numpuy array like n x m\n",
        "        returns:\n",
        "        numpy array with stacked column of 1s in 0th column\"\"\"\n",
        "\n",
        "        c = np.hstack((np.ones((input_matrix.shape[0], 1)) , input_matrix))\n",
        "        return c\n",
        "    \n",
        "    # loss function\n",
        "    def cost(self, ): \n",
        "        \"\"\" Calculates the cost of current configuration\"\"\"\n",
        "        \n",
        "        y_cap = self.X.dot(self.thetas)  # all zeroes\n",
        "        \n",
        "        # (1/rows)*(y`-y)^2 mean squared error\n",
        "        return (1 / (2 * self.n_rows)) * np.sum((y_cap - self.y) ** 2)\n",
        "    \n",
        "    \n",
        "    def predict(self, new_X):\n",
        "        new_X = self._hstack_one(new_X)\n",
        "        return new_X.dot(self.thetas)\n",
        "    \n",
        "    \n",
        "    def batch_gradient(self, ):\n",
        "        y_cap = self.X.dot(self.thetas) # Y` = X.0\n",
        "        h = y_cap - self.y\n",
        "        h = np.multiply(self.X, h) # theta_1  --> see next cell for, more multiplicaion details of this\n",
        "        h = np.sum(h, axis = 0) # taking the summation\n",
        "        #print(\"printing h: \\n\")\n",
        "        #print(h)\n",
        "        h = h.reshape(-1, 1) # now we have the two thetas\n",
        "        \n",
        "        #this function will return \n",
        "        # y` = X(dot)theta\n",
        "        #(sum((y`-y)^2)*X)\n",
        "        return h\n",
        "        \n",
        "        \n",
        "    # doing the gradient descent process\n",
        "    def batch_gradient_descent(self, ):\n",
        "        alpha_by_m = self.alpha / self.n_rows\n",
        "        for i in range(self.n_iterations):\n",
        "            \"\"\"going by the rul oe batchgradient descent\n",
        "            calculate all the gradient, then update the thetas\n",
        "            \"\"\"\n",
        "            # J(theta) = J(theta) - (leanring_rate/rows)(dj/dj(theta))\n",
        "            self.thetas = self.thetas - (alpha_by_m * self.batch_gradient()) \n",
        "            cost = self.cost() # cacl the loss now\n",
        "            if i%1000 == 0:\n",
        "                print (f\"Epoch: {i}, loss: {cost:.3f}\")\n",
        "               # print(\"epoch: {0} Loss: {1: .5f}\\r\".format(i+1, cost), end=\"\")  \n",
        "                #print(\"Iteration: {0} Loss: {1: .5f}\\r\".format(i+1, cost), end=\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZLfeTbaVtujU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.dtypes\n",
        "dataset.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EazLgfEqtujY",
        "colab_type": "text"
      },
      "source": [
        "### Need to normalize data as so much difference between values.\n",
        "\n",
        "refer : https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff\n",
        "\n",
        ">  def standardize_data(data, mean, std):\n",
        "> >      return (data - mean)/std"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UhZaHHlmtujY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler() \n",
        "data_scaled = scaler.fit_transform(dataset)\n",
        "data_scaled[0]\n",
        "\n",
        "# def standardize_data(data, mean, std):\n",
        "#     return (data - mean)/std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FJETzyRktuje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data_scaled[:, 0]\n",
        "y = data_scaled[:, 1]\n",
        "\n",
        "print(X[0])\n",
        "print(y[0])\n",
        "print(y.shape)\n",
        "print(X.shape)\n",
        "\n",
        "X = X.reshape(-1, 1)\n",
        "y = y.reshape(-1, 1)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rNsjoobutuji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b-_601L9tujl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LinearRegression(X, y)\n",
        "lr.batch_gradient_descent()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S7WHHWRXtujo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing\n",
        "print(X[0])\n",
        "print(lr.predict(X)[0])\n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Au_821potujr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X[0: 20]\n",
        "y_train = y[0: 20]\n",
        "X_test = X[21: 41]\n",
        "y_test = y[21: 41]\n",
        "plt.scatter(X_train, y_train)\n",
        "\n",
        "pred_train = lr.predict(X_train)\n",
        "pred_test = lr.predict(X_test)\n",
        "\n",
        "for i in range(10):\n",
        "    print(X_train[i], \" \", y_train[i] , \" \", pred_train[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PSK-5TAItujv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and test MSE\n",
        "train_mse = np.mean((y_train - pred_train) ** 2)\n",
        "test_mse = np.mean((y_test - pred_test) ** 2)\n",
        "print (f\"train_MSE: {train_mse:.2f}, test_MSE: {test_mse:.2f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nLCi4165tujy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "# Plot train data\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plt.scatter(X_train, y_train, label='y_train')\n",
        "plt.plot(X_train, pred_train, color='red', linewidth=1, linestyle='-', label='model')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Plot test data\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plt.scatter(X_test, y_test, label='y_test')\n",
        "plt.plot(X_test, pred_test, color='red', linewidth=1, linestyle='-', label='model')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Show plots\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs2Ba8n7tuj5",
        "colab_type": "text"
      },
      "source": [
        "#### Conclusion:\n",
        "Since the dataset is very small, only 60 rows, increasing to epochs to >5000 is giving good predictions."
      ]
    }
  ]
}